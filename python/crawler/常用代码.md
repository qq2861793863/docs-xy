# 常用代码（筱宇整理）

## xy_tools

```python
import requests,parsel,csv,os
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'
}

def xy_req_xpath(base_url,xpath):
    base_url = base_url
    response = requests.get(url=base_url, headers=headers)
    html = parsel.Selector(response.text)
    list = html.xpath(xpath)
    return list
# 检测当前链接是否有效
def xy_check_url(url, timeout=2):
    try:
        response = requests.get(url, headers, timeout=timeout)
        return url if response.status_code == 200 else "无效链接"
    except requests.exceptions.Timeout:
        return "请求超时"
    except requests.exceptions.ConnectionError:
        return "连接错误"
    except requests.exceptions.RequestException as e:
        return f"请求异常: {e}"

def xy_save_csv(name='data.csv',fieldnames=[]):
    f = open(name, mode='a', encoding='utf-8-sig', newline='')
    csv_writer = csv.DictWriter(f, fieldnames)
    csv_writer.writeheader()
    return csv_writer


def created_dir(parent_directory,dir_name):
    full_path = os.path.join(parent_directory, dir_name)
    if not os.path.exists(full_path):
        os.mkdir(full_path)
        print(f'{full_path} 文件夹创建成功')
```

## 万能解码

```python
response.encoding=response.apparent_encoding
```

## 创建文件夹

```python
正则匹配文件名自动创建
dir_name=re.findall('<h1>'+str(name)+'</h1>',html)[-1]
    if not os.path.exists(dir_name):
        os.mkdir(dir_name)
#创建根目录

filename = "pdf\\"
if not os.path.exists(filename ):#判断创建的目录是否存在，如果不存在则创建
    os.mkdir(filename )
```

## 过滤切割

```python
#过滤切割
strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。
注意：该方法只能删除开头或是结尾的字符，不能删除中间部分的字符。
count_id=song_id.strip('/song?id=')
# 过滤含有$的每一项
result='$' in count_id
if ('$' in count_id) == False:
```

## 读写内容

```python
#读内容
with open('文件名','r',encoding='utf-8') as f:
 text=f.readlines()
 print(text)
#写内容
with open('1.txt','w',encoding='utf-8') as f:
 text='123'
 f.write(text)
 f.write('筱宇测试')
```

## json数据

```python
读取请求json数据
data=respones.json()
解析数据
data_list=data['subjects']   键对应的值
pprint.pprint(data_list)  格式化输出
循环一一读取
for li in  data_list:
    title=li['title'] 
```

## 判断

```python
判断
if title:
else:
        print('没找到')
```

## 数据写入表格

```python
with open('文件名.csv.csv',mode='a',encoding='utf-8',newline='') as f:   newline='' 新行写入  换行
        csv_write=csv.writer(f)
        csv_write.writerow([title,time])  把所有想要的字段构建成列表 序列化
```

## 巧用

```python
time.sleep(1)  # 延迟1秒

len（）查看个数
len巧用：
for li in range(1,len(变量名)+1):

异常捕获
    try:

    except Exception as e:
        
if input("按回车键继续..."):
            pass
    
占位符
print('正在下载第{}个套图'.format(i))
print(f'正在下载第{变量名}个套图')

爬虫：
urls = [f'网址:{i}' for i in range(1, 10)]
print(urls)
for url in urls:
    print(url)
    
    
areas=[i.strip() for i in areas]
for index in zip(href,areas):
    index[0]

    
def change(title):
    #替换字符串
    #re.compile()编译
    #re.sub替换  把特殊字符串都替换成_
    #把替换后的内容返回
   mode=re.compile(r'[\/\\\:\*\?\"\<\>\|]')
   new_title=re.sub(mode,'_',title)
   return new_title

内容替换
html_str.format(article=content)
```

## 下载保存

```python
    title = list.split("/")[-1]
    response = requests.get(url=img_url, headers=headers).content
    with open("img\\" + title, 'wb') as f:
        f.write(response)
        print(f'{title}下载成功')
```

# 爬虫相关

## 解析套用

```python
import requests,parsel
headers={
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'
}
base_url=
response=requests.get(url=base_url,headers=headers)
html=parsel.Selector(response.text)
lis=html.xpath('')
for li in lis:
    
    
#解码套用
response=requests.get(url=url, headers=headers)
response.encoding = response.apparent_encoding
data=parsel.Selector(response.text)
```

## 表格套用

```python
f=open('data.csv',mode='a',encoding='utf-8',newline='')
csv_writer=csv.DictWriter(f,fieldnames=['标题','地区'])
csv_writer.writeheader()
for 循环:
   dit={
        '标题':title,
        '地区':area,
    }
    csv_writer.writerow(dit)
```

## 多线程套用

```python
from multiprocessing.dummy import Pool
href = []
正常循环：
        dic = {
            'url': url,
            'title': title
        }
        href.append(dic)
def get_img(dic):
    url=dic['url']
    title=dic['title']
    response = requests.get(url).content
    with open('img\\' + title, 'wb')as f:
        f.write(response)
    print(f'{title}下载成功')
# 实例化一个线程池对象
pool=Pool(4)
# 将列表中的每一个列表元素传递给get_page进行处理
# 参数1:函数  参数二:带入函数的列表  多线程类似于循环往里带
pool.map(get_img,href)
pool.close()
pool.join()
```

## 智能化取名

```python
url=''
data = parsel.Selector(requests.get(url=url, headers=headers).text)
name=data.xpath('').get()
num=name.split('/')[-1].split(')')[0]
print(f'当前套图为:{name}------一共{num}张图片')
url=url.split('.html')[0]  #解析地址按照实际情况写
for page in range(1,int(num)+1):
    base_url=f'{url}_{page}.html'#解析地址按照实际情况写
    respones=requests.get(url=base_url,headers=headers)
```

## 头部信息

```python
头部信息
headers={
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'
}
```

## re解析

```python
html=response.text
dir_name=re.findall('<h2>:(.*?)</h2>',html)
re.findall('<img class=".*?" src="(.*?)" .*? />',html)  #需要的东西放到(.*?)
```

## xpath解析

```python
xpath解析（.extract()  .getall()获取多个 .get() .extract_frist()获取一个 ）
1.
import parsel
html=parsel.Selector(response.text)
text=html.xpath('//ul[@id="homeLastBox"]/li[@class="home-last half-50"]')
for a in text:
    title=a.xpath('./a/text()').get()
------------------------------------------------------------------------------------------------
2.from lxml import etree
selector=etree.HTML(html)
lists=selector.xpath('//div[@id="content-innerText"]//img/@src')
-------------------------------------------------------------------------------------------------
3.sel=parsel.Selector(response.text)
h1=sel.css('h1::text')
content=sel.css('#content::text')
title=h1.get()
lines=content.getall()
```

## 转pdf

```python
html_str="""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8">
    <title>Document</title>
</head>  
<body>
{article}
</body>
</html>
"""
def change(title):
    #替换字符串
    #re.compile()编译
    #re.sub替换  把特殊字符串都替换成_
    #把替换后的内容返回
   mode=re.compile(r'[\/\\\:\*\?\"\<\>\|]')
   new_title=re.sub(mode,'_',title)
   return new_title

filename = "pdf\\"
if not os.path.exists(filename ):#判断创建的目录是否存在，如果不存在则创建
    os.mkdir(filename )
    
    html=html_str.format(article=content)
    new_title = change(title)
    html_path=filename+new_title+'.html'
    pdf_path=filename+new_title+'.pdf'
    with open(html_path,mode='w',encoding='utf-8')as f:
        f.write(html)
        print('正在保存',title)
    path_wkthmltopdf = r'D:\\软件\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'
    config = pdfkit.configuration(wkhtmltopdf=path_wkthmltopdf)
    pdfkit.from_url(html_path, pdf_path, configuration=config)
```



## 模拟登录

```python
import requests
# 会话
session=requests.session()
data={
"loginName": "13213269343",
"password": "5605538lzy"
}
# 1.登陆
# url='https://passport.17k.com/ck/user/login'
# session.post(url=url,data=data)
# resp=session.post(url=url,data=data)
# print(resp.cookies) #查看cookies
# 2.拿书架上的书
# 刚才的session中是有cookies的
# resp=session.get('https://user.17k.com/ck/author/shelf?page=1&appKey=2406394919')
# print(resp.json())
-----------------------------------------------------------------------------------------
headers={
"Cookie": "GUID=436debc8-7e94-4eb9-9203-54d611376a67; c_channel=0; c_csc=web; accessToken=avatarUrl%3Dhttps%253A%252F%252Fcdn.static.17k.com%252Fuser%252Favatar%252F14%252F74%252F06%252F77660674.jpg-88x88%253Fv%253D1623755204000%26id%3D77660674%26nickname%3D%25E7%25AD%25B1%25E5%25AE%2587765%26e%3D1639307252%26s%3Dff6a7e11abfa73df"
}
resp=requests.get('https://user.17k.com/ck/author/shelf?page=1&appKey=2406394919',headers=headers)
print(resp.text)
```

## 反扒机制

```python
请求头  防盗链  cookies
headers={
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36',
    # 防盗链
         'Referer': 'https://www.pearvideo.com/video_1732242'
}
```

## 代理

```python
import requests
# ip:端口
proxies={
'https':'https://120.76.135.236:39846'
}
resp=requests.get('https://fanyi.baidu.com/',proxies=proxies)
print(resp.text)
```

## scrapy框架 

1. scrapy的安装：pip install scrapy
2. 创建scrapy的项目: scrapy startproject myspider
3. 创建scrapy爬虫：在项目目录下执行 scrapy genspider itcast itcast.cn
4. 运行scrapy爬虫：在项目目录下执行 scrapy crawl itcast
5. 解析并获取scrapy爬虫中的数据：
   1. response.xpath方法的返回结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有一些额外的方法
   2. extract() 返回一个包含有字符串的列表
   3. extract_first() 返回列表中的第一个字符串，列表为空没有返回None
6. scrapy管道的基本使用:
   1. 完善pipelines.py中的process_item函数
   2. 在settings.py中设置开启pipeline
7. response响应对象的常用属性
   1. response.url：当前响应的url地址
   2. response.request.url：当前响应对应的请求的url地址
   3. response.headers：响应头
   4. response.requests.headers：当前响应的请求头
   5. response.body：响应体，也就是html代码，byte类型
   6. response.status：响应状态码

```python
设置:
LOG_LEVEL='WARNING' #不打印日志
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'
ROBOTSTXT_OBEY = False
#CONCURRENT_REQUESTS = 32  开启32个线程  默认16个
#COOKIES_ENABLED = False  开启cookie
```

### **持久化存储**

```python
基于终端指令的持久化存储

这种方法只可以将parse方法的返回值存储到本地指定后缀的文件文本中（不可以写入数据库  局限性高）
将列表中的数据写入到csv文件中
scrapy crawl 爬虫名称 -o data.csv

示例代码:
class MeizhuoSpider(scrapy.Spider):
    name = 'meizhuo'
    allowed_domains = ['win4000.com']
    start_urls = ['http://www.win4000.com/mobile_2341_0_0_1.html']
    def parse(self, response):
        # print(response.body)
        list=[]
        urls=response.xpath('//div[@class="tab_box"]//ul[@class="clearfix"]/li')
        for url in urls:
            title = url.xpath('.//p/text()').get()
            url=url.xpath('./a/@href').get()
            # print(title,url,sep='|')
            dic={
                'title':title,
                'url':url
            } 
            list.append(dic)
        return list
```

```python
基于管道的持久化存储（重点）

1.在爬虫文件中进行数据解析
2.在items.py中定义相关属性
    步骤1中解析除了几个字段的数据，再次就定义几个属性
    title = scrapy.Field()
    url = scrapy.Field()

3.在爬虫文件中 将解析到的数据存储到item类型的对象中
from Myscrapy.items import MyscrapyItem
# 实例化一个item类型的对象，将解析到的数据存储到该对象中
            item=MyscrapyItem()
            # 不可以通过点的形式调用属性
            item['title']=title
            item['url']=url
 
5.将item类型的对象提交给管道
            yield item

6.在管道文件（pipelines.py）中接收爬虫文件提交过来的item类型的对象，且对其进行任意形式的持久化存储
class MyscrapyPipeline:
    fp=None
    # 重写父类的两个方法
    def open_spider(self,spider):
        print('我是open_spider（），我只会在爬虫开始的时候执行一次')
        self.fp=open('data.txt','w',encoding='utf-8')
    def close_spider(self, spider):
        print('我是open_spider（），我只会在爬虫结束的时候执行一次')
        self.fp.close()
    # 该方法是用来接收item对象  一次只能接收一个item  说明该方法会被调用多次
    # 参数item:就是接收到的item对象
    def process_item(self, item, spider):
        # print(item)  #item就是一个字典
        # 将item存储到文本文件中
        self.fp.write(item['title']+':'+item['url']+'\n')
        return item
    
7.在配置文件中打开管道机制
ITEM_PIPELINES = {
   # 300表示管道类的优先级  数值越小优先级越高
   'Myscrapy.pipelines.MyscrapyPipeline': 300,

}
```

基于管道实现数据的备份（Myscrapy）

将爬取到的数据分别存储到不同的载体。

数据储存到不同的载体就需要使用多个管道类

item会不会依次提交给三个管道类

​      不会 ，爬虫文件中的item只会提交给优先级最高的那一个管道类

​       优先级高的管道类需要在process_item中实现**return  item**  就item传递给下一个即将被执行的管道类

**scrapy的手动发送请求实现的全站数据爬取**（handReqPro）

```
yield scrapy.Request(url,callback):GET
yield scrapy.FormRequest(url,callback,formdata):POST
formdata:字典 请求参数
```

callback指定解析函数  用于解析数据

### **请求传参实现的深度爬取** 

项目：（moviepro）

深度爬取：爬取到的数据没有在同一张页面中（首页数据+详情页数据）

在scrapy中如果没有请求传参我们是无法持久化存储的

实现方式：

scrapy.Request（url，callback，meta）

meta是一个字典，可以将meta传递给callback

callback取出meta：

response.meta[]

### **中间键**

项目：（middlePro）

作用：批量拦截请求和响应

爬虫中间件

下载中间件（推荐）

拦截请求：篡改请求url   伪装请求头信息 （UA cookie） 设置代理（重点）

拦截响应：篡改响应数据

cookie在scrapy 不需要设置   

process_exception设置代理

代理操作必须使用中间件才可以实现

process_exception：request.meta['proxy']='http://ip:port'#设置代理

```python
class MiddleproDownloaderMiddleware:

    #拦截所有的请求(正常  异常)
    #request就是拦截到的请求，spider就是爬虫类实例化的对象
    def process_request(self, request, spider):
        print('process_request()')
        return None
    #拦截所有的响应对象
    # 参数: response拦截到的响应对象，request响应对象的请求对象
    def process_response(self, request, response, spider):
        print('process_response()')
        return response
     #拦截异常的请求
    #参数：request就是拦截到的发生异常的请求
    #作用:想要将异常的请求进行修正，将其编程正常的请求，然后对其进行重新发送
    def process_exception(self, request, exception, spider):
        #请求的ip被禁掉，请求就会变成一个异常的请求
        # request.meta['proxy']='http://ip:port'#设置代理
        print('process_exception()')
        # return request#将异常的请求修正后将其进行重新发送
```

### 大文件下载

（imgPro）

下载管道类是scrapy封装好的我们直接用即可

```
from scrapy.pipelines.images import ImagesPipeline #提供了数据下载功能
```

重写管道三个类

```python
class ImgPipiLine(ImagesPipeline):
    #根据图片地址发起请求
    def get_media_requests(self, item, info):
        yield scrapy.Request(url=item['src'],meta={'item':item})

     #返回图片名称即可
    def file_path(self, request, response=None, info=None):
        item=request.meta['item']
        filePath=item['name']
        return filePath #只需要返回图片名称
    #将item传递给下一个即将被执行的管道类
    def item_completed(self, results, item, info):
        return item
```

在配置文件中添加图片保存位置：

```
IMAGES_STORE='./img'
```

settings.py中常用的设置：

（可以适当增加爬虫爬取效率）

增加并发:

默认scrapy开启的并发线程为32个，可以适当进行增加。在settings配置文件中修改CONCURRENT_REQUESTS = 100值为100 ,并发设置成了为100。

降低日志级别:

在运行scrapy时，会有大量日志信息的输出，为了减少CPU的使用率。可以设置log输出信息为INFO或者ERROR即可。在配置文件中编写: LOG_LEVEL = ' INFO'

禁止cookie:

如果不是真的需要cookie,则在scrapy爬取数据时可以禁l止cookie从而减少CPU的使用率，提升爬取效率。在配置文件中编写: COOKIES ENABLED = False

禁止重试:

对失败的HTTP进行重新请求(重试)会减慢爬取速度，因此可以禁止重试。在配置文件中编写: RETRY_ ENABLED = False

减少下载超时:

如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写: DOwNL0AD TIMEOUT = 10超时时间为10s

### CrawlSpider

**CrawlSpider实现深度爬取（sunPro）：**

**通用方式：CrawlSpider+Spider实现**

其实是spider的一个子类  spider爬虫文件中爬虫类的父类。

子类的功能一定是多于父类

作用：被作用于专业实现全站数据爬取

将一个页面下所有的页码对应的数据进行爬取

基本使用：

1.创建一个工程   2.cd工程  

3.创建一个基于CrawlSpider的爬虫文件

scrapy genspider -t crawl SpiderName www.xxx.com

4.执行工程

注意：一个链接提取器对应一个规则解析器（多个链接提取器和多个规则解析器）

在实现深度爬取的过程中需要和scrapy.Request()结合使用

面试题：如何将一个网站中所有的链接都进行爬取

```python
link=LinkExtractor(allow=r'')
follow=True
```

### seleuium在scrcapy中的使用

爬取网易新闻中国内 国际、军事、航空、无人机这五个板块下所有的新闻数据（标题+内容）

分布式

增量式

### 遇到的问题（redis）

ModuleNotFoundError: **No module named redis**

在安装过Redis后，通过Python程序导入redis时，遇到一个“ImportError: No module named redis”错误，网上查了下原因，解决办法如下：

Python默认是不支持Redis的，当引用redis时就会报错

这里需要为Python安装Redis库，登陆https://github.com/andymccurdy/redis-py 后点击Download ZIP下载安装包。解压

找到解压好的redis-py目录 

安装redis-py：

```bash
C:\Users\Administrator>cd redis-py

C:\Users\Administrator\redis-py>python setup.py instal
```