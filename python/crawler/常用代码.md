# å¸¸ç”¨ä»£ç ï¼ˆç­±å®‡æ•´ç†ï¼‰

## xy_tools

```python
import requests,parsel,csv,os
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'
}

def xy_req_xpath(base_url,xpath):
    base_url = base_url
    response = requests.get(url=base_url, headers=headers)
    html = parsel.Selector(response.text)
    list = html.xpath(xpath)
    return list
# æ£€æµ‹å½“å‰é“¾æ¥æ˜¯å¦æœ‰æ•ˆ
def xy_check_url(url, timeout=2):
    try:
        response = requests.get(url, headers, timeout=timeout)
        return url if response.status_code == 200 else "æ— æ•ˆé“¾æ¥"
    except requests.exceptions.Timeout:
        return "è¯·æ±‚è¶…æ—¶"
    except requests.exceptions.ConnectionError:
        return "è¿æ¥é”™è¯¯"
    except requests.exceptions.RequestException as e:
        return f"è¯·æ±‚å¼‚å¸¸: {e}"

def xy_save_csv(name='data.csv',fieldnames=[]):
    f = open(name, mode='a', encoding='utf-8-sig', newline='')
    csv_writer = csv.DictWriter(f, fieldnames)
    csv_writer.writeheader()
    return csv_writer


def created_dir(parent_directory,dir_name):
    full_path = os.path.join(parent_directory, dir_name)
    if not os.path.exists(full_path):
        os.mkdir(full_path)
        print(f'{full_path} æ–‡ä»¶å¤¹åˆ›å»ºæˆåŠŸ')
        
def email_send(text,toName):
    # ç™»å½•
    smtp_obj = smtplib.SMTP_SSL('smtp.qq.com', 465)
    smtp_obj.login('2861793863@qq.com', 'ojkfessrfztrdedj')
    msg = MIMEText(f'æ•°æ®å†…å®¹:\n{text}', 'plain', 'utf-8')
    # è®¾ç½®é‚®ä»¶å¤´éƒ¨
    msg['From'] = '2861793863@qq.com'  # å‘é€è€…é‚®ç®±åœ°å€
    msg['Subject'] = Header('ç­±å®‡python', 'utf-8')  # ä¸»é¢˜
    # å‘é€
    smtp_obj.sendmail('2861793863@qq.com',toName, msg.as_string())
    print('é‚®ä»¶å‘é€æˆåŠŸ')
    # æ–­å¼€è¿æ¥
    smtp_obj.quit()
    # email_send('æµ‹è¯•',['1921047422@qq.com'])
```

## ä¸‡èƒ½è§£ç 

```python
response.encoding=response.apparent_encoding
```

## åˆ›å»ºæ–‡ä»¶å¤¹

```python
æ­£åˆ™åŒ¹é…æ–‡ä»¶åè‡ªåŠ¨åˆ›å»º
dir_name=re.findall('<h1>'+str(name)+'</h1>',html)[-1]
    if not os.path.exists(dir_name):
        os.mkdir(dir_name)
#åˆ›å»ºæ ¹ç›®å½•

filename = "pdf\\"
if not os.path.exists(filename ):#åˆ¤æ–­åˆ›å»ºçš„ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
    os.mkdir(filename )
```

## è¿‡æ»¤åˆ‡å‰²

```python
#è¿‡æ»¤åˆ‡å‰²
strip() æ–¹æ³•ç”¨äºç§»é™¤å­—ç¬¦ä¸²å¤´å°¾æŒ‡å®šçš„å­—ç¬¦ï¼ˆé»˜è®¤ä¸ºç©ºæ ¼æˆ–æ¢è¡Œç¬¦ï¼‰æˆ–å­—ç¬¦åºåˆ—ã€‚
æ³¨æ„ï¼šè¯¥æ–¹æ³•åªèƒ½åˆ é™¤å¼€å¤´æˆ–æ˜¯ç»“å°¾çš„å­—ç¬¦ï¼Œä¸èƒ½åˆ é™¤ä¸­é—´éƒ¨åˆ†çš„å­—ç¬¦ã€‚
count_id=song_id.strip('/song?id=')
# è¿‡æ»¤å«æœ‰$çš„æ¯ä¸€é¡¹
result='$' in count_id
if ('$' in count_id) == False:
```

## è¯»å†™å†…å®¹

```python
#è¯»å†…å®¹
with open('æ–‡ä»¶å','r',encoding='utf-8') as f:
 text=f.readlines()
 print(text)
#å†™å†…å®¹
with open('1.txt','w',encoding='utf-8') as f:
 text='123'
 f.write(text)
 f.write('ç­±å®‡æµ‹è¯•')
```

## jsonæ•°æ®

```python
è¯»å–è¯·æ±‚jsonæ•°æ®
data=respones.json()
è§£ææ•°æ®
data_list=data['subjects']   é”®å¯¹åº”çš„å€¼
pprint.pprint(data_list)  æ ¼å¼åŒ–è¾“å‡º
å¾ªç¯ä¸€ä¸€è¯»å–
for li in  data_list:
    title=li['title'] 
```

## åˆ¤æ–­

```python
åˆ¤æ–­
if title:
else:
        print('æ²¡æ‰¾åˆ°')
```

## æ•°æ®å†™å…¥è¡¨æ ¼

```python
with open('æ–‡ä»¶å.csv.csv',mode='a',encoding='utf-8',newline='') as f:   newline='' æ–°è¡Œå†™å…¥  æ¢è¡Œ
        csv_write=csv.writer(f)
        csv_write.writerow([title,time])  æŠŠæ‰€æœ‰æƒ³è¦çš„å­—æ®µæ„å»ºæˆåˆ—è¡¨ åºåˆ—åŒ–
```

## å·§ç”¨

```python
time.sleep(1)  # å»¶è¿Ÿ1ç§’

lenï¼ˆï¼‰æŸ¥çœ‹ä¸ªæ•°
lenå·§ç”¨ï¼š
for li in range(1,len(å˜é‡å)+1):

å¼‚å¸¸æ•è·
    try:

    except Exception as e:
        
if input("æŒ‰å›è½¦é”®ç»§ç»­..."):
            pass
    
å ä½ç¬¦
print('æ­£åœ¨ä¸‹è½½ç¬¬{}ä¸ªå¥—å›¾'.format(i))
print(f'æ­£åœ¨ä¸‹è½½ç¬¬{å˜é‡å}ä¸ªå¥—å›¾')

çˆ¬è™«ï¼š
urls = [f'ç½‘å€:{i}' for i in range(1, 10)]
print(urls)
for url in urls:
    print(url)
    
    
areas=[i.strip() for i in areas]
for index in zip(href,areas):
    index[0]

    
def change(title):
    #æ›¿æ¢å­—ç¬¦ä¸²
    #re.compile()ç¼–è¯‘
    #re.subæ›¿æ¢  æŠŠç‰¹æ®Šå­—ç¬¦ä¸²éƒ½æ›¿æ¢æˆ_
    #æŠŠæ›¿æ¢åçš„å†…å®¹è¿”å›
   mode=re.compile(r'[\/\\\:\*\?\"\<\>\|]')
   new_title=re.sub(mode,'_',title)
   return new_title

å†…å®¹æ›¿æ¢
html_str.format(article=content)
```

## ä¸‹è½½ä¿å­˜

```python
    title = list.split("/")[-1]
    response = requests.get(url=img_url, headers=headers).content
    with open("img\\" + title, 'wb') as f:
        f.write(response)
        print(f'{title}ä¸‹è½½æˆåŠŸ')
```

# çˆ¬è™«ç›¸å…³

## è§£æå¥—ç”¨

```python
import requests,parsel
headers={
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'
}
base_url=
response=requests.get(url=base_url,headers=headers)
html=parsel.Selector(response.text)
lis=html.xpath('')
for li in lis:
    
    
#è§£ç å¥—ç”¨
response=requests.get(url=url, headers=headers)
response.encoding = response.apparent_encoding
data=parsel.Selector(response.text)
```

## è¡¨æ ¼å¥—ç”¨

```python
f=open('data.csv',mode='a',encoding='utf-8',newline='')
csv_writer=csv.DictWriter(f,fieldnames=['æ ‡é¢˜','åœ°åŒº'])
csv_writer.writeheader()
for å¾ªç¯:
   dit={
        'æ ‡é¢˜':title,
        'åœ°åŒº':area,
    }
    csv_writer.writerow(dit)
```

## å¤šçº¿ç¨‹å¥—ç”¨

```python
from multiprocessing.dummy import Pool
href = []
æ­£å¸¸å¾ªç¯ï¼š
        dic = {
            'url': url,
            'title': title
        }
        href.append(dic)
def get_img(dic):
    url=dic['url']
    title=dic['title']
    response = requests.get(url).content
    with open('img\\' + title, 'wb')as f:
        f.write(response)
    print(f'{title}ä¸‹è½½æˆåŠŸ')
# å®ä¾‹åŒ–ä¸€ä¸ªçº¿ç¨‹æ± å¯¹è±¡
pool=Pool(4)
# å°†åˆ—è¡¨ä¸­çš„æ¯ä¸€ä¸ªåˆ—è¡¨å…ƒç´ ä¼ é€’ç»™get_pageè¿›è¡Œå¤„ç†
# å‚æ•°1:å‡½æ•°  å‚æ•°äºŒ:å¸¦å…¥å‡½æ•°çš„åˆ—è¡¨  å¤šçº¿ç¨‹ç±»ä¼¼äºå¾ªç¯å¾€é‡Œå¸¦
pool.map(get_img,href)
pool.close()
pool.join()
```

## æ™ºèƒ½åŒ–å–å

```python
url=''
data = parsel.Selector(requests.get(url=url, headers=headers).text)
name=data.xpath('').get()
num=name.split('/')[-1].split(')')[0]
print(f'å½“å‰å¥—å›¾ä¸º:{name}------ä¸€å…±{num}å¼ å›¾ç‰‡')
url=url.split('.html')[0]  #è§£æåœ°å€æŒ‰ç…§å®é™…æƒ…å†µå†™
for page in range(1,int(num)+1):
    base_url=f'{url}_{page}.html'#è§£æåœ°å€æŒ‰ç…§å®é™…æƒ…å†µå†™
    respones=requests.get(url=base_url,headers=headers)
```

## å¤´éƒ¨ä¿¡æ¯

```python
å¤´éƒ¨ä¿¡æ¯
headers={
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'
}
```

## reè§£æ

```python
html=response.text
dir_name=re.findall('<h2>:(.*?)</h2>',html)
re.findall('<img class=".*?" src="(.*?)" .*? />',html)  #éœ€è¦çš„ä¸œè¥¿æ”¾åˆ°(.*?)
```

## xpathè§£æ

```python
xpathè§£æï¼ˆ.extract()  .getall()è·å–å¤šä¸ª .get() .extract_frist()è·å–ä¸€ä¸ª ï¼‰
1.
import parsel
html=parsel.Selector(response.text)
text=html.xpath('//ul[@id="homeLastBox"]/li[@class="home-last half-50"]')
for a in text:
    title=a.xpath('./a/text()').get()
------------------------------------------------------------------------------------------------
2.from lxml import etree
selector=etree.HTML(html)
lists=selector.xpath('//div[@id="content-innerText"]//img/@src')
-------------------------------------------------------------------------------------------------
3.sel=parsel.Selector(response.text)
h1=sel.css('h1::text')
content=sel.css('#content::text')
title=h1.get()
lines=content.getall()
```

## è½¬pdf

```python
html_str="""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8">
    <title>Document</title>
</head>  
<body>
{article}
</body>
</html>
"""
def change(title):
    #æ›¿æ¢å­—ç¬¦ä¸²
    #re.compile()ç¼–è¯‘
    #re.subæ›¿æ¢  æŠŠç‰¹æ®Šå­—ç¬¦ä¸²éƒ½æ›¿æ¢æˆ_
    #æŠŠæ›¿æ¢åçš„å†…å®¹è¿”å›
   mode=re.compile(r'[\/\\\:\*\?\"\<\>\|]')
   new_title=re.sub(mode,'_',title)
   return new_title

filename = "pdf\\"
if not os.path.exists(filename ):#åˆ¤æ–­åˆ›å»ºçš„ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
    os.mkdir(filename )
    
    html=html_str.format(article=content)
    new_title = change(title)
    html_path=filename+new_title+'.html'
    pdf_path=filename+new_title+'.pdf'
    with open(html_path,mode='w',encoding='utf-8')as f:
        f.write(html)
        print('æ­£åœ¨ä¿å­˜',title)
    path_wkthmltopdf = r'D:\\è½¯ä»¶\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'
    config = pdfkit.configuration(wkhtmltopdf=path_wkthmltopdf)
    pdfkit.from_url(html_path, pdf_path, configuration=config)
```



## æ¨¡æ‹Ÿç™»å½•

```python
import requests
# ä¼šè¯
session=requests.session()
data={
"loginName": "13213269343",
"password": "5605538lzy"
}
# 1.ç™»é™†
# url='https://passport.17k.com/ck/user/login'
# session.post(url=url,data=data)
# resp=session.post(url=url,data=data)
# print(resp.cookies) #æŸ¥çœ‹cookies
# 2.æ‹¿ä¹¦æ¶ä¸Šçš„ä¹¦
# åˆšæ‰çš„sessionä¸­æ˜¯æœ‰cookiesçš„
# resp=session.get('https://user.17k.com/ck/author/shelf?page=1&appKey=2406394919')
# print(resp.json())
-----------------------------------------------------------------------------------------
headers={
"Cookie": "GUID=436debc8-7e94-4eb9-9203-54d611376a67; c_channel=0; c_csc=web; accessToken=avatarUrl%3Dhttps%253A%252F%252Fcdn.static.17k.com%252Fuser%252Favatar%252F14%252F74%252F06%252F77660674.jpg-88x88%253Fv%253D1623755204000%26id%3D77660674%26nickname%3D%25E7%25AD%25B1%25E5%25AE%2587765%26e%3D1639307252%26s%3Dff6a7e11abfa73df"
}
resp=requests.get('https://user.17k.com/ck/author/shelf?page=1&appKey=2406394919',headers=headers)
print(resp.text)
```

## åæ‰’æœºåˆ¶

```python
è¯·æ±‚å¤´  é˜²ç›—é“¾  cookies
headers={
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36',
    # é˜²ç›—é“¾
         'Referer': 'https://www.pearvideo.com/video_1732242'
}
```

## ä»£ç†

```python
import requests
# ip:ç«¯å£
proxies={
'https':'https://120.76.135.236:39846'
}
resp=requests.get('https://fanyi.baidu.com/',proxies=proxies)
print(resp.text)
```

## drissionpage

```
http://drissionpage.cn/

DrissionPageÂ® æ˜¯ä¸€ä¸ªåŸºäº python çš„ç½‘é¡µè‡ªåŠ¨åŒ–å·¥å…·ã€‚
å®ƒæ—¢èƒ½æ§åˆ¶æµè§ˆå™¨ï¼Œä¹Ÿèƒ½æ”¶å‘æ•°æ®åŒ…ï¼Œè¿˜èƒ½æŠŠä¸¤è€…åˆè€Œä¸ºä¸€ã€‚
å¯å…¼é¡¾æµè§ˆå™¨è‡ªåŠ¨åŒ–çš„ä¾¿åˆ©æ€§å’Œ requests çš„é«˜æ•ˆç‡ã€‚
å®ƒåŠŸèƒ½å¼ºå¤§ï¼Œå†…ç½®æ— æ•°äººæ€§åŒ–è®¾è®¡å’Œä¾¿æ·åŠŸèƒ½ã€‚
å®ƒçš„è¯­æ³•ç®€æ´è€Œä¼˜é›…ï¼Œä»£ç é‡å°‘ï¼Œå¯¹æ–°æ‰‹å‹å¥½ã€‚

pip install DrissionPage

âœ…ï¸ï¸ å‡çº§
ğŸ“Œ å‡çº§æœ€æ–°ç¨³å®šç‰ˆ
pip install DrissionPage --upgrade

ğŸ“Œ æŒ‡å®šç‰ˆæœ¬å‡çº§
pip install DrissionPage==4.0.0b17

è¿è¡Œç¯å¢ƒ
æ“ä½œç³»ç»Ÿï¼šWindowsã€Linux å’Œ Macã€‚

python ç‰ˆæœ¬ï¼š3.6 åŠä»¥ä¸Š

æ”¯æŒæµè§ˆå™¨ï¼šChromium å†…æ ¸ï¼ˆå¦‚ Chrome å’Œ Edgeï¼‰
```

```python
æŠ“å–çŸ¥ä¹ä¿å­˜excel
`
from DrissionPage import Chromium, ChromiumOptions
import time
from DataRecorder import Recorder
recorder = Recorder('data.csv')
co = ChromiumOptions().use_system_user_path()
# å¯åŠ¨æˆ–æ¥ç®¡æµè§ˆå™¨ï¼Œå¹¶åˆ›å»ºæ ‡ç­¾é¡µå¯¹è±¡
tab = Chromium(co).latest_tab
# è·³è½¬åˆ°ç™»å½•é¡µé¢
tab.get('https://www.zhihu.com/')
time.sleep(3)
list = tab.eles('x://h2[@class="ContentItem-title"]') # ç®€åŒ–å†™æ³•
for li in list:
    url=li.ele('x://a').attr('href')
    text=li.ele('x://a/text()')
    recorder.add_data((text,url))
recorder.record()
```

## selenium

```
seleniumå·¥å…·é¢„å¤‡
Chromeæµè§ˆå™¨
Chromeé©±åŠ¨
é©±åŠ¨ä¸‹è½½ç½‘å€:
[115ç‰ˆæœ¬åŠä»¥ä¸Š]https://googlechromelabs.github.io/chrome-for-testing/
[114ç‰ˆæœ¬åŠä»¥ä¸‹]https://registry.npmmirror.com/binary.html?path=chromedriver/

é©±åŠ¨ç‰ˆæœ¬é€‰æ‹©ç‰ˆæœ¬é€‰æ‹©
è°·æ­Œæµè§ˆå™¨åœ°å€æ è¾“å…¥: chrome://version
é©±åŠ¨æ ¹æ®æµè§ˆå™¨ç‰ˆæœ¬å‰ä¸‰ä½æ¥è¿›è¡Œé€‰æ‹© (64-32ä½éƒ½å¯ä»¥å…¼å®¹)
ä¸ªäººèµ„æ–™è·¯å¾„(ç¼“å­˜è·¯å¾„)ä¹Ÿéœ€è¦ä½¿ç”¨
é©±åŠ¨ä¿å­˜åˆ°è°·æ­Œæµè§ˆå™¨çš„å®‰è£…ç›®å½•ä¸‹
å°†è°·æ­Œçš„å®‰è£…è·¯å¾„æ·»åŠ åˆ°ç¯å¢ƒå˜é‡



æ— æ³•è¢«ä¾¦æµ‹çš„ä»£ç†é©±åŠ¨
å¦‚æœéœ€è¦ä½¿ç”¨è¿™ä¸ªæ¨¡å¼éœ€è¦æŠŠseleinumç‰ˆæœ¬æ§åˆ¶ä¸è¶…è¿‡4.12çš„ç‰ˆæœ¬
å¹¶ä¸æ˜¯æ‰€æœ‰çš„åŒå­¦éƒ½å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ¨¡å—ã€‚
undetected chromedriver
pip install undetected chromedriverd
```

## scrapyæ¡†æ¶ 

1. scrapyçš„å®‰è£…ï¼špip install scrapy
2. åˆ›å»ºscrapyçš„é¡¹ç›®: scrapy startproject myspider
3. åˆ›å»ºscrapyçˆ¬è™«ï¼šåœ¨é¡¹ç›®ç›®å½•ä¸‹æ‰§è¡Œ scrapy genspider itcast itcast.cn
4. è¿è¡Œscrapyçˆ¬è™«ï¼šåœ¨é¡¹ç›®ç›®å½•ä¸‹æ‰§è¡Œ scrapy crawl itcast
5. è§£æå¹¶è·å–scrapyçˆ¬è™«ä¸­çš„æ•°æ®ï¼š
   1. response.xpathæ–¹æ³•çš„è¿”å›ç»“æœæ˜¯ä¸€ä¸ªç±»ä¼¼listçš„ç±»å‹ï¼Œå…¶ä¸­åŒ…å«çš„æ˜¯selectorå¯¹è±¡ï¼Œæ“ä½œå’Œåˆ—è¡¨ä¸€æ ·ï¼Œä½†æ˜¯æœ‰ä¸€äº›é¢å¤–çš„æ–¹æ³•
   2. extract() è¿”å›ä¸€ä¸ªåŒ…å«æœ‰å­—ç¬¦ä¸²çš„åˆ—è¡¨
   3. extract_first() è¿”å›åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œåˆ—è¡¨ä¸ºç©ºæ²¡æœ‰è¿”å›None
6. scrapyç®¡é“çš„åŸºæœ¬ä½¿ç”¨:
   1. å®Œå–„pipelines.pyä¸­çš„process_itemå‡½æ•°
   2. åœ¨settings.pyä¸­è®¾ç½®å¼€å¯pipeline
7. responseå“åº”å¯¹è±¡çš„å¸¸ç”¨å±æ€§
   1. response.urlï¼šå½“å‰å“åº”çš„urlåœ°å€
   2. response.request.urlï¼šå½“å‰å“åº”å¯¹åº”çš„è¯·æ±‚çš„urlåœ°å€
   3. response.headersï¼šå“åº”å¤´
   4. response.requests.headersï¼šå½“å‰å“åº”çš„è¯·æ±‚å¤´
   5. response.bodyï¼šå“åº”ä½“ï¼Œä¹Ÿå°±æ˜¯htmlä»£ç ï¼Œbyteç±»å‹
   6. response.statusï¼šå“åº”çŠ¶æ€ç 

```python
è®¾ç½®:
LOG_LEVEL='WARNING' #ä¸æ‰“å°æ—¥å¿—
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'
ROBOTSTXT_OBEY = False
#CONCURRENT_REQUESTS = 32  å¼€å¯32ä¸ªçº¿ç¨‹  é»˜è®¤16ä¸ª
#COOKIES_ENABLED = False  å¼€å¯cookie
```

### **æŒä¹…åŒ–å­˜å‚¨**

```python
åŸºäºç»ˆç«¯æŒ‡ä»¤çš„æŒä¹…åŒ–å­˜å‚¨

è¿™ç§æ–¹æ³•åªå¯ä»¥å°†parseæ–¹æ³•çš„è¿”å›å€¼å­˜å‚¨åˆ°æœ¬åœ°æŒ‡å®šåç¼€çš„æ–‡ä»¶æ–‡æœ¬ä¸­ï¼ˆä¸å¯ä»¥å†™å…¥æ•°æ®åº“  å±€é™æ€§é«˜ï¼‰
å°†åˆ—è¡¨ä¸­çš„æ•°æ®å†™å…¥åˆ°csvæ–‡ä»¶ä¸­
scrapy crawl çˆ¬è™«åç§° -o data.csv

ç¤ºä¾‹ä»£ç :
class MeizhuoSpider(scrapy.Spider):
    name = 'meizhuo'
    allowed_domains = ['win4000.com']
    start_urls = ['http://www.win4000.com/mobile_2341_0_0_1.html']
    def parse(self, response):
        # print(response.body)
        list=[]
        urls=response.xpath('//div[@class="tab_box"]//ul[@class="clearfix"]/li')
        for url in urls:
            title = url.xpath('.//p/text()').get()
            url=url.xpath('./a/@href').get()
            # print(title,url,sep='|')
            dic={
                'title':title,
                'url':url
            } 
            list.append(dic)
        return list
```

```python
åŸºäºç®¡é“çš„æŒä¹…åŒ–å­˜å‚¨ï¼ˆé‡ç‚¹ï¼‰

1.åœ¨çˆ¬è™«æ–‡ä»¶ä¸­è¿›è¡Œæ•°æ®è§£æ
2.åœ¨items.pyä¸­å®šä¹‰ç›¸å…³å±æ€§
    æ­¥éª¤1ä¸­è§£æé™¤äº†å‡ ä¸ªå­—æ®µçš„æ•°æ®ï¼Œå†æ¬¡å°±å®šä¹‰å‡ ä¸ªå±æ€§
    title = scrapy.Field()
    url = scrapy.Field()

3.åœ¨çˆ¬è™«æ–‡ä»¶ä¸­ å°†è§£æåˆ°çš„æ•°æ®å­˜å‚¨åˆ°itemç±»å‹çš„å¯¹è±¡ä¸­
from Myscrapy.items import MyscrapyItem
# å®ä¾‹åŒ–ä¸€ä¸ªitemç±»å‹çš„å¯¹è±¡ï¼Œå°†è§£æåˆ°çš„æ•°æ®å­˜å‚¨åˆ°è¯¥å¯¹è±¡ä¸­
            item=MyscrapyItem()
            # ä¸å¯ä»¥é€šè¿‡ç‚¹çš„å½¢å¼è°ƒç”¨å±æ€§
            item['title']=title
            item['url']=url
 
5.å°†itemç±»å‹çš„å¯¹è±¡æäº¤ç»™ç®¡é“
            yield item

6.åœ¨ç®¡é“æ–‡ä»¶ï¼ˆpipelines.pyï¼‰ä¸­æ¥æ”¶çˆ¬è™«æ–‡ä»¶æäº¤è¿‡æ¥çš„itemç±»å‹çš„å¯¹è±¡ï¼Œä¸”å¯¹å…¶è¿›è¡Œä»»æ„å½¢å¼çš„æŒä¹…åŒ–å­˜å‚¨
class MyscrapyPipeline:
    fp=None
    # é‡å†™çˆ¶ç±»çš„ä¸¤ä¸ªæ–¹æ³•
    def open_spider(self,spider):
        print('æˆ‘æ˜¯open_spiderï¼ˆï¼‰ï¼Œæˆ‘åªä¼šåœ¨çˆ¬è™«å¼€å§‹çš„æ—¶å€™æ‰§è¡Œä¸€æ¬¡')
        self.fp=open('data.txt','w',encoding='utf-8')
    def close_spider(self, spider):
        print('æˆ‘æ˜¯open_spiderï¼ˆï¼‰ï¼Œæˆ‘åªä¼šåœ¨çˆ¬è™«ç»“æŸçš„æ—¶å€™æ‰§è¡Œä¸€æ¬¡')
        self.fp.close()
    # è¯¥æ–¹æ³•æ˜¯ç”¨æ¥æ¥æ”¶itemå¯¹è±¡  ä¸€æ¬¡åªèƒ½æ¥æ”¶ä¸€ä¸ªitem  è¯´æ˜è¯¥æ–¹æ³•ä¼šè¢«è°ƒç”¨å¤šæ¬¡
    # å‚æ•°item:å°±æ˜¯æ¥æ”¶åˆ°çš„itemå¯¹è±¡
    def process_item(self, item, spider):
        # print(item)  #itemå°±æ˜¯ä¸€ä¸ªå­—å…¸
        # å°†itemå­˜å‚¨åˆ°æ–‡æœ¬æ–‡ä»¶ä¸­
        self.fp.write(item['title']+':'+item['url']+'\n')
        return item
    
7.åœ¨é…ç½®æ–‡ä»¶ä¸­æ‰“å¼€ç®¡é“æœºåˆ¶
ITEM_PIPELINES = {
   # 300è¡¨ç¤ºç®¡é“ç±»çš„ä¼˜å…ˆçº§  æ•°å€¼è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜
   'Myscrapy.pipelines.MyscrapyPipeline': 300,

}
```

åŸºäºç®¡é“å®ç°æ•°æ®çš„å¤‡ä»½ï¼ˆMyscrapyï¼‰

å°†çˆ¬å–åˆ°çš„æ•°æ®åˆ†åˆ«å­˜å‚¨åˆ°ä¸åŒçš„è½½ä½“ã€‚

æ•°æ®å‚¨å­˜åˆ°ä¸åŒçš„è½½ä½“å°±éœ€è¦ä½¿ç”¨å¤šä¸ªç®¡é“ç±»

itemä¼šä¸ä¼šä¾æ¬¡æäº¤ç»™ä¸‰ä¸ªç®¡é“ç±»

â€‹      ä¸ä¼š ï¼Œçˆ¬è™«æ–‡ä»¶ä¸­çš„itemåªä¼šæäº¤ç»™ä¼˜å…ˆçº§æœ€é«˜çš„é‚£ä¸€ä¸ªç®¡é“ç±»

â€‹       ä¼˜å…ˆçº§é«˜çš„ç®¡é“ç±»éœ€è¦åœ¨process_itemä¸­å®ç°**return  item**  å°±itemä¼ é€’ç»™ä¸‹ä¸€ä¸ªå³å°†è¢«æ‰§è¡Œçš„ç®¡é“ç±»

**scrapyçš„æ‰‹åŠ¨å‘é€è¯·æ±‚å®ç°çš„å…¨ç«™æ•°æ®çˆ¬å–**ï¼ˆhandReqProï¼‰

```
yield scrapy.Request(url,callback):GET
yield scrapy.FormRequest(url,callback,formdata):POST
formdata:å­—å…¸ è¯·æ±‚å‚æ•°
```

callbackæŒ‡å®šè§£æå‡½æ•°  ç”¨äºè§£ææ•°æ®

### **è¯·æ±‚ä¼ å‚å®ç°çš„æ·±åº¦çˆ¬å–** 

é¡¹ç›®ï¼šï¼ˆmovieproï¼‰

æ·±åº¦çˆ¬å–ï¼šçˆ¬å–åˆ°çš„æ•°æ®æ²¡æœ‰åœ¨åŒä¸€å¼ é¡µé¢ä¸­ï¼ˆé¦–é¡µæ•°æ®+è¯¦æƒ…é¡µæ•°æ®ï¼‰

åœ¨scrapyä¸­å¦‚æœæ²¡æœ‰è¯·æ±‚ä¼ å‚æˆ‘ä»¬æ˜¯æ— æ³•æŒä¹…åŒ–å­˜å‚¨çš„

å®ç°æ–¹å¼ï¼š

scrapy.Requestï¼ˆurlï¼Œcallbackï¼Œmetaï¼‰

metaæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå¯ä»¥å°†metaä¼ é€’ç»™callback

callbackå–å‡ºmetaï¼š

response.meta[]

### **ä¸­é—´é”®**

é¡¹ç›®ï¼šï¼ˆmiddleProï¼‰

ä½œç”¨ï¼šæ‰¹é‡æ‹¦æˆªè¯·æ±‚å’Œå“åº”

çˆ¬è™«ä¸­é—´ä»¶

ä¸‹è½½ä¸­é—´ä»¶ï¼ˆæ¨èï¼‰

æ‹¦æˆªè¯·æ±‚ï¼šç¯¡æ”¹è¯·æ±‚url   ä¼ªè£…è¯·æ±‚å¤´ä¿¡æ¯ ï¼ˆUA cookieï¼‰ è®¾ç½®ä»£ç†ï¼ˆé‡ç‚¹ï¼‰

æ‹¦æˆªå“åº”ï¼šç¯¡æ”¹å“åº”æ•°æ®

cookieåœ¨scrapy ä¸éœ€è¦è®¾ç½®   

process_exceptionè®¾ç½®ä»£ç†

ä»£ç†æ“ä½œå¿…é¡»ä½¿ç”¨ä¸­é—´ä»¶æ‰å¯ä»¥å®ç°

process_exceptionï¼šrequest.meta['proxy']='http://ip:port'#è®¾ç½®ä»£ç†

```python
class MiddleproDownloaderMiddleware:

    #æ‹¦æˆªæ‰€æœ‰çš„è¯·æ±‚(æ­£å¸¸  å¼‚å¸¸)
    #requestå°±æ˜¯æ‹¦æˆªåˆ°çš„è¯·æ±‚ï¼Œspiderå°±æ˜¯çˆ¬è™«ç±»å®ä¾‹åŒ–çš„å¯¹è±¡
    def process_request(self, request, spider):
        print('process_request()')
        return None
    #æ‹¦æˆªæ‰€æœ‰çš„å“åº”å¯¹è±¡
    # å‚æ•°: responseæ‹¦æˆªåˆ°çš„å“åº”å¯¹è±¡ï¼Œrequestå“åº”å¯¹è±¡çš„è¯·æ±‚å¯¹è±¡
    def process_response(self, request, response, spider):
        print('process_response()')
        return response
     #æ‹¦æˆªå¼‚å¸¸çš„è¯·æ±‚
    #å‚æ•°ï¼šrequestå°±æ˜¯æ‹¦æˆªåˆ°çš„å‘ç”Ÿå¼‚å¸¸çš„è¯·æ±‚
    #ä½œç”¨:æƒ³è¦å°†å¼‚å¸¸çš„è¯·æ±‚è¿›è¡Œä¿®æ­£ï¼Œå°†å…¶ç¼–ç¨‹æ­£å¸¸çš„è¯·æ±‚ï¼Œç„¶åå¯¹å…¶è¿›è¡Œé‡æ–°å‘é€
    def process_exception(self, request, exception, spider):
        #è¯·æ±‚çš„ipè¢«ç¦æ‰ï¼Œè¯·æ±‚å°±ä¼šå˜æˆä¸€ä¸ªå¼‚å¸¸çš„è¯·æ±‚
        # request.meta['proxy']='http://ip:port'#è®¾ç½®ä»£ç†
        print('process_exception()')
        # return request#å°†å¼‚å¸¸çš„è¯·æ±‚ä¿®æ­£åå°†å…¶è¿›è¡Œé‡æ–°å‘é€
```

### å¤§æ–‡ä»¶ä¸‹è½½

ï¼ˆimgProï¼‰

ä¸‹è½½ç®¡é“ç±»æ˜¯scrapyå°è£…å¥½çš„æˆ‘ä»¬ç›´æ¥ç”¨å³å¯

```
from scrapy.pipelines.images import ImagesPipeline #æä¾›äº†æ•°æ®ä¸‹è½½åŠŸèƒ½
```

é‡å†™ç®¡é“ä¸‰ä¸ªç±»

```python
class ImgPipiLine(ImagesPipeline):
    #æ ¹æ®å›¾ç‰‡åœ°å€å‘èµ·è¯·æ±‚
    def get_media_requests(self, item, info):
        yield scrapy.Request(url=item['src'],meta={'item':item})

     #è¿”å›å›¾ç‰‡åç§°å³å¯
    def file_path(self, request, response=None, info=None):
        item=request.meta['item']
        filePath=item['name']
        return filePath #åªéœ€è¦è¿”å›å›¾ç‰‡åç§°
    #å°†itemä¼ é€’ç»™ä¸‹ä¸€ä¸ªå³å°†è¢«æ‰§è¡Œçš„ç®¡é“ç±»
    def item_completed(self, results, item, info):
        return item
```

åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ å›¾ç‰‡ä¿å­˜ä½ç½®ï¼š

```
IMAGES_STORE='./img'
```

settings.pyä¸­å¸¸ç”¨çš„è®¾ç½®ï¼š

ï¼ˆå¯ä»¥é€‚å½“å¢åŠ çˆ¬è™«çˆ¬å–æ•ˆç‡ï¼‰

å¢åŠ å¹¶å‘:

é»˜è®¤scrapyå¼€å¯çš„å¹¶å‘çº¿ç¨‹ä¸º32ä¸ªï¼Œå¯ä»¥é€‚å½“è¿›è¡Œå¢åŠ ã€‚åœ¨settingsé…ç½®æ–‡ä»¶ä¸­ä¿®æ”¹CONCURRENT_REQUESTS = 100å€¼ä¸º100 ,å¹¶å‘è®¾ç½®æˆäº†ä¸º100ã€‚

é™ä½æ—¥å¿—çº§åˆ«:

åœ¨è¿è¡Œscrapyæ—¶ï¼Œä¼šæœ‰å¤§é‡æ—¥å¿—ä¿¡æ¯çš„è¾“å‡ºï¼Œä¸ºäº†å‡å°‘CPUçš„ä½¿ç”¨ç‡ã€‚å¯ä»¥è®¾ç½®logè¾“å‡ºä¿¡æ¯ä¸ºINFOæˆ–è€…ERRORå³å¯ã€‚åœ¨é…ç½®æ–‡ä»¶ä¸­ç¼–å†™: LOG_LEVEL = ' INFO'

ç¦æ­¢cookie:

å¦‚æœä¸æ˜¯çœŸçš„éœ€è¦cookie,åˆ™åœ¨scrapyçˆ¬å–æ•°æ®æ—¶å¯ä»¥ç¦læ­¢cookieä»è€Œå‡å°‘CPUçš„ä½¿ç”¨ç‡ï¼Œæå‡çˆ¬å–æ•ˆç‡ã€‚åœ¨é…ç½®æ–‡ä»¶ä¸­ç¼–å†™: COOKIES ENABLED = False

ç¦æ­¢é‡è¯•:

å¯¹å¤±è´¥çš„HTTPè¿›è¡Œé‡æ–°è¯·æ±‚(é‡è¯•)ä¼šå‡æ…¢çˆ¬å–é€Ÿåº¦ï¼Œå› æ­¤å¯ä»¥ç¦æ­¢é‡è¯•ã€‚åœ¨é…ç½®æ–‡ä»¶ä¸­ç¼–å†™: RETRY_ ENABLED = False

å‡å°‘ä¸‹è½½è¶…æ—¶:

å¦‚æœå¯¹ä¸€ä¸ªéå¸¸æ…¢çš„é“¾æ¥è¿›è¡Œçˆ¬å–ï¼Œå‡å°‘ä¸‹è½½è¶…æ—¶å¯ä»¥èƒ½è®©å¡ä½çš„é“¾æ¥å¿«é€Ÿè¢«æ”¾å¼ƒï¼Œä»è€Œæå‡æ•ˆç‡ã€‚åœ¨é…ç½®æ–‡ä»¶ä¸­è¿›è¡Œç¼–å†™: DOwNL0AD TIMEOUT = 10è¶…æ—¶æ—¶é—´ä¸º10s

### CrawlSpider

**CrawlSpiderå®ç°æ·±åº¦çˆ¬å–ï¼ˆsunProï¼‰ï¼š**

**é€šç”¨æ–¹å¼ï¼šCrawlSpider+Spiderå®ç°**

å…¶å®æ˜¯spiderçš„ä¸€ä¸ªå­ç±»  spiderçˆ¬è™«æ–‡ä»¶ä¸­çˆ¬è™«ç±»çš„çˆ¶ç±»ã€‚

å­ç±»çš„åŠŸèƒ½ä¸€å®šæ˜¯å¤šäºçˆ¶ç±»

ä½œç”¨ï¼šè¢«ä½œç”¨äºä¸“ä¸šå®ç°å…¨ç«™æ•°æ®çˆ¬å–

å°†ä¸€ä¸ªé¡µé¢ä¸‹æ‰€æœ‰çš„é¡µç å¯¹åº”çš„æ•°æ®è¿›è¡Œçˆ¬å–

åŸºæœ¬ä½¿ç”¨ï¼š

1.åˆ›å»ºä¸€ä¸ªå·¥ç¨‹   2.cdå·¥ç¨‹  

3.åˆ›å»ºä¸€ä¸ªåŸºäºCrawlSpiderçš„çˆ¬è™«æ–‡ä»¶

scrapy genspider -t crawl SpiderName www.xxx.com

4.æ‰§è¡Œå·¥ç¨‹

æ³¨æ„ï¼šä¸€ä¸ªé“¾æ¥æå–å™¨å¯¹åº”ä¸€ä¸ªè§„åˆ™è§£æå™¨ï¼ˆå¤šä¸ªé“¾æ¥æå–å™¨å’Œå¤šä¸ªè§„åˆ™è§£æå™¨ï¼‰

åœ¨å®ç°æ·±åº¦çˆ¬å–çš„è¿‡ç¨‹ä¸­éœ€è¦å’Œscrapy.Request()ç»“åˆä½¿ç”¨

é¢è¯•é¢˜ï¼šå¦‚ä½•å°†ä¸€ä¸ªç½‘ç«™ä¸­æ‰€æœ‰çš„é“¾æ¥éƒ½è¿›è¡Œçˆ¬å–

```python
link=LinkExtractor(allow=r'')
follow=True
```

### seleuiumåœ¨scrcapyä¸­çš„ä½¿ç”¨

çˆ¬å–ç½‘æ˜“æ–°é—»ä¸­å›½å†… å›½é™…ã€å†›äº‹ã€èˆªç©ºã€æ— äººæœºè¿™äº”ä¸ªæ¿å—ä¸‹æ‰€æœ‰çš„æ–°é—»æ•°æ®ï¼ˆæ ‡é¢˜+å†…å®¹ï¼‰

åˆ†å¸ƒå¼

å¢é‡å¼

### é‡åˆ°çš„é—®é¢˜ï¼ˆredisï¼‰

ModuleNotFoundError: **No module named redis**

åœ¨å®‰è£…è¿‡Redisåï¼Œé€šè¿‡Pythonç¨‹åºå¯¼å…¥redisæ—¶ï¼Œé‡åˆ°ä¸€ä¸ªâ€œImportError: No module named redisâ€é”™è¯¯ï¼Œç½‘ä¸ŠæŸ¥äº†ä¸‹åŸå› ï¼Œè§£å†³åŠæ³•å¦‚ä¸‹ï¼š

Pythoné»˜è®¤æ˜¯ä¸æ”¯æŒRedisçš„ï¼Œå½“å¼•ç”¨redisæ—¶å°±ä¼šæŠ¥é”™

è¿™é‡Œéœ€è¦ä¸ºPythonå®‰è£…Redisåº“ï¼Œç™»é™†https://github.com/andymccurdy/redis-py åç‚¹å‡»Download ZIPä¸‹è½½å®‰è£…åŒ…ã€‚è§£å‹

æ‰¾åˆ°è§£å‹å¥½çš„redis-pyç›®å½• 

å®‰è£…redis-pyï¼š

```bash
C:\Users\Administrator>cd redis-py

C:\Users\Administrator\redis-py>python setup.py instal
```

# æ¡ˆä¾‹

## å°é¹¿çº¿

```python
import requests,parsel,re
headers={
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36',
    'Cookie':'SESSION=ZjY1MzMwYjUtMGVmNi00YmI3LWJhNjctZjdkZjBiMGMyMDA2',
    'Referer': 'https://www.xuexiluxian.cn/'
}
i=1
def download_file(url, filename):
    # å‘é€ GET è¯·æ±‚
    response = requests.get(url, headers=headers,stream=True)
    # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
    if response.status_code == 200:
        # æ‰“å¼€æ–‡ä»¶ä»¥å†™å…¥äºŒè¿›åˆ¶æ•°æ®
        with open(f'video\\{filename}', 'wb') as f:
            # ä»¥å—çš„å½¢å¼å†™å…¥æ–‡ä»¶
            for chunk in response.iter_content(chunk_size=1024 * 10 * 1024):
                f.write(chunk)
        print(f'File downloaded successfully: {filename}')
    else:
        print(f'Failed to download file: {response.status_code}')


base_url='https://www.xuexiluxian.cn/course/detail/f1e91e8f4a5e43c59bc2badf0bcc790b'
response=requests.get(url=base_url,headers=headers)
html=parsel.Selector(response.text)
lis=html.xpath('//a[@class="btn-learn"]/@onclick')
for li in lis:
    # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é… URL
    pattern = r"('/player/play\?courseId=.*?&chapterId=.*?')"
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡ŒåŒ¹é…
    match = re.search(pattern, li.get())
    # æå–åŒ¹é…åˆ°çš„ URL
    if match:
        # æå–åˆ°çš„ URL
        relative_url = match.group(1).strip("'")  # è·å–ç¬¬ä¸€ä¸ªåˆ†ç»„
        # æ‹¼æ¥å®Œæ•´çš„ URL
        full_url = f"https://www.xuexiluxian.cn{relative_url}"
        response = requests.get(url=full_url, headers=headers)
        video_url=re.findall(r'"playURL":"(https?://[^\s"]+\.mp4)"',response.text)[0]
        print(video_url)
        video_title=re.findall('<span class="name">(.*?)</span>',response.text)[0]
        download_file(video_url,f'{str(i)}.{video_title}.mp4')
        i+=1
```

## é‚®ä»¶å‘é€

```python
from xy_tools import email_send
email_send('æµ‹è¯•',['2861793863@qq.com'])
```

### ç™¾åº¦æ–°é—»

```python
import requests,parsel,pymysql,schedule,time
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36',
}
def baidu():
    # è¿æ¥æ•°æ®åº“
    connection = pymysql.connect(
        host='39.106.253.148',  # æ›¿æ¢ä¸ºä½ çš„æ•°æ®åº“ä¸»æœºåœ°å€
        database='news',  # æ›¿æ¢ä¸ºä½ çš„æ•°æ®åº“å
        user='news',  # æ›¿æ¢ä¸ºä½ çš„æ•°æ®åº“ç”¨æˆ·å
        password='',  # æ›¿æ¢ä¸ºä½ çš„æ•°æ®åº“å¯†ç 
        charset='utf8mb4',  # ç¡®ä¿å­—ç¬¦é›†æ”¯æŒ
        cursorclass=pymysql.cursors.DictCursor  # ä½¿ç”¨å­—å…¸æ¸¸æ ‡
    )
    response = requests.get('https://top.baidu.com/board?tab=realtime', headers=headers)
    list = parsel.Selector(response.text).xpath('//div[@class="category-wrap_iQLoo horizontal_1eKyQ"]')
    for li in list:
        src = li.xpath(".//img/@src").get()
        title = li.xpath(".//div[@class='c-single-text-ellipsis']/text()").get()
        text = li.xpath(".//div[@class='content_1YWBm']/div[2]/text()").get().strip()
        hot = li.xpath(".//div[@class='hot-index_1Bl1a']/text()").get()
        try:
            with connection.cursor() as cursor:
                sql = "INSERT INTO news (title,src,text, hot) VALUES (%s, %s, %s, %s)"
                cursor.execute(sql, (title, src, text, hot))
                connection.commit()
        except pymysql.MySQLError as e:
            print(f"Error: {e}")
    connection.close()
schedule.every().day.at("11:00").do(baidu)

while True:
    schedule.run_pending()
    time.sleep(1)
```
